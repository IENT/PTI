{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Header](img/header_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Creating a Binaural Synthesis of a Moving Sound Source\n",
    "---\n",
    "In this part, the topic of binaural synthesis is investigated further by simulating a moving sound source. Therefore, a trajectory of a starting airplane is created in cartesian coordinates.\n",
    "\n",
    "The corresponding HRIRs are selected and the cross-fading between the HRIRs are implemented. Finally, the plausibility of the generated signals are investigated and evaluated.\n",
    "\n",
    "To get an impression of the resulting auralization, listen to the final auralization result using headphones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(\"audio/auralization_with_doppler_shift.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Trajectory of a starting airplane\n",
    "---\n",
    "In this task, a trajectory of a starting airplane is created in cartesian coordinates.\n",
    "### Task 1.1: Creating the trajectory\n",
    "---\n",
    "Facing a moving sound source, the HRIRs need to be switched at defined times. Hence, it is necessary to introduce block-wise processing as well as cross-fading between those blocks. Each block with a number of samples `blocksize` corresponds to a shared position in the trajectory of the airplane.\n",
    "The sampling rate `sampling_rate`, the duration of the simulated signal `duration`, the blocksize `blocksize`, the total number of blocks `n_blocks` and the neccessary positions are already defined in the cell below.\n",
    "\n",
    "The speed of the airplane `speed`, the speed of sound `c`, the point of takeoff `start_point`, a time vector `time_vector_blocks`, containing the discrete time steps of the discrete positions, and also a time vector `time_vector` containing the discrete time steps of each sample, are already given. One meter corresponds to one unit.\n",
    "\n",
    "Define the path of the starting airplane in cartesian coordinates by defining a vector in direction of the velocity `velocity`, and by calculating the source positions `src_pos` for each block using a linear function\n",
    "\n",
    "$$\\begin{pmatrix}p_x\\\\p_x\\\\p_z\\end{pmatrix}=\\begin{pmatrix}v_x\\\\v_y\\\\v_z\\end{pmatrix}t+ \\begin{pmatrix}b_x\\\\b_y\\\\b_z\\end{pmatrix}$$\n",
    "\n",
    "with $\\vec{p}$ denoting the position for each block, $\\vec{v}$ the speed of the aircraft, $t$ the time instance for the respective block and $\\vec{b}$ the start position of the aircraft.\n",
    "\n",
    "*Hint: You need to use `np.outer(...)` ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.outer.html)) to perform the outer product of two vectors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sofa\n",
    "import helper_functions as hf\n",
    "\n",
    "# import HRIR dataset:\n",
    "HRIR_path = \"hrir/ITA_Artificial_Head_5x5_44100Hz.sofa\"\n",
    "HRIR_dataset = sofa.Database.open(HRIR_path)\n",
    "\n",
    "# define global settings\n",
    "sampling_rate = 44100\n",
    "duration = 20 # seconds\n",
    "total_input_samples = duration*sampling_rate\n",
    "\n",
    "# define block size\n",
    "blocksize = 1024*4\n",
    "n_blocks = int(np.ceil(total_input_samples/blocksize))\n",
    "\n",
    "# define speed of the airplane in meter per seconds\n",
    "speed = 350 * 1000/3600\n",
    "c = 343 # speed of sound\n",
    "\n",
    "# define the time vector, containing the discrete time instances of each block\n",
    "time_vector_blocks = np.arange(0,duration,blocksize/sampling_rate)\n",
    "time_vector = np.arange(0,duration,1/sampling_rate)\n",
    "\n",
    "# define start position of moving source\n",
    "start_point = np.array((50,-1000,0))\n",
    "\n",
    "# unit vector in direction of moving source:\n",
    "direction = np.array((0,1,0.05))\n",
    "direction /= np.linalg.norm(direction)\n",
    "\n",
    "# define position, view- and up-vector of receiver\n",
    "rec_pos = np.array([0,0,1.8])\n",
    "rec_view_vector = np.squeeze(HRIR_dataset.Listener.View.get_values(system=\"cartesian\"))\n",
    "rec_up_vector = np.squeeze(HRIR_dataset.Listener.Up.get_values(system=\"cartesian\"))\n",
    "\n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "# vector in direction of velocity:\n",
    "# velocity = ...\n",
    "\n",
    "# calculate positions via linear function (s=v*t)\n",
    "# src_pos = ...\n",
    "\n",
    "###### ! Solution ends here ! ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Visualizing the trajectory\n",
    "---\n",
    "To get a visual interpretation of the situation, execute the cell below. The source positions for all time instances including the listener's view and up vector are shown in the resulting figure.\n",
    "\n",
    "*Note: You are not supposed do do any implementation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# create a new figure\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# plot the positions of the source and the receiver\n",
    "ax.scatter(src_pos[:,0], src_pos[:,1],src_pos[:,2], label='Trajectory') # Students\n",
    "ax.scatter(rec_pos[0],rec_pos[1],rec_pos[2], label='Receiver') # Students\n",
    "\n",
    "# plot the view and up vector of the receiver\n",
    "ax.quiver(rec_pos[0], rec_pos[1], rec_pos[2],\n",
    "           rec_view_vector[0]*10, rec_view_vector[1]*10, rec_view_vector[2]*10,\n",
    "           color='red', label='View vector')\n",
    "ax.quiver(rec_pos[0], rec_pos[1], rec_pos[2],\n",
    "           rec_up_vector[0]*20, rec_up_vector[1]*20, rec_up_vector[2]*20,\n",
    "           color='green', label='Up vector')\n",
    "\n",
    "# set the labels accordingly and activate legend\n",
    "ax.set_xlabel('x [m]')\n",
    "ax.set_ylabel('y [m]')\n",
    "ax.set_zlabel('z [m]')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Calculating the distances, azimuth and elevation angles\n",
    "---\n",
    "In the following, we will assume, that the airplane can be represented by a point source. The position of the source in space is defined by the trajectory above. In order to create a auralization of an airplane fly-by, we will need to include the perceived loudness change due to the distance variation and the correct HRTF for the angles corresponding to the airplane's position relative to the receiver. The preceived loudness change is due to the reduction of sound pressure according to $p(r) \\sim 1/r$, that is the inverse distance law.\n",
    "In order to apply the inverse distance law and also to select the respective HRIR for each block, the distances and azimuth/elevation angles have to be calculated.\n",
    "### Task 2.1: Angle between two vectors\n",
    "---\n",
    "Implement the function `angle_between_vectors(v1, v2, deg=True)`.\n",
    "For calculating an angle between two vectors, the following dependencies hold true:\n",
    "$$ \\cos(\\theta)  = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}| |\\vec{b}|}$$\n",
    "\n",
    "$$ \\sin(\\theta)  = \\frac{\\vec{a} \\times \\vec{b}}{|\\vec{a}| |\\vec{b}|}$$\n",
    "\n",
    "$$ \\tan(\\theta)  = \\frac{\\sin(\\theta)}{\\cos(\\theta)}$$\n",
    "\n",
    "*Hint: It is not sufficient to only use the dot product here, as the sign of the angle is of interest. Use all three equations above for the calculation!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_vectors(v1, v2, deg=True):\n",
    "    \"\"\"\n",
    "    Returns the angle of two n-dimensional vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v1 : numpy.ndarray\n",
    "        First vector.\n",
    "    v2 : numpy.ndarray\n",
    "        Second vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    angle : numpy.ndarray\n",
    "        The angle between the given vectors.\n",
    "    \"\"\"\n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "    # ...\n",
    "        \n",
    "###### ! Solution ends here ! ######\n",
    "    if deg:\n",
    "        angle = np.degrees(angle)\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Calculating the distances, azimuth and elevation angles\n",
    "---\n",
    "In order to apply the inverse distance law (1/r) and also to select the respective HRIR for each block, the distances and azimuth/elevation angles have to be calculated.\n",
    "\n",
    "1. Calculate the distances of the receiver and the positions of the aircraft using the euclidean norm, provided by `np.linalg.norm()` ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)) and store them in the array `distances`.\n",
    "\n",
    "2. Use the previously implemented function `angle_between_vectors(v1, v2)` to calculate the azimuth and elevation angles beween the up/view-vector and the source position vectors. A simple way to do this is to project the vectors to the xy- and xz-plane respectively and use the resulting two-dimensional vectors to calculate the angle. Store the results in the arrays `azimuth` and `elevation`.\n",
    "\n",
    "*Note: The resulting angles are only valid for the static receiver position. Due to the scope of this lab, a valid implementation of the rotation of the listener is omitted.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ! Solution begins here ! ######\n",
    "\n",
    "# calculate the distances of the source and receiver for each block\n",
    "# ...\n",
    "\n",
    "# projection of vectors to the xy and yz plane\n",
    "# ...\n",
    "\n",
    "# calculate the azimuth and elevation angles using the prevously implemented function\n",
    "# azimuth = ...\n",
    "# elevation = ...\n",
    "\n",
    "###### ! Solution ends here ! ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Defining the excitation signal\n",
    "---\n",
    "In this task, the excitation signal of the source/airplane is defined. Take a look at the excitation's spectrogram and listen to it using the widget below. Describe the components of the excitiation signal.\n",
    "\n",
    "*Note: You are not supposed do do any implementation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "# define the excitation signal as a mixture of white Gaussian noise\n",
    "# and a sine mixture with a fundamental frequency of 880 Hz and some harmonics\n",
    "f0 = 880\n",
    "alpha = 0.2\n",
    "excitation = (np.random.standard_normal(total_input_samples)\n",
    "              + alpha * np.sin(2*np.pi*f0*time_vector)\n",
    "              + alpha * np.sin(2*np.pi*2*f0*time_vector)\n",
    "              + alpha * np.sin(2*np.pi*3*f0*time_vector)\n",
    "              + alpha * np.sin(2*np.pi*4*f0*time_vector)\n",
    "              + alpha * np.sin(2*np.pi*5*f0*time_vector)\n",
    "              + alpha * np.sin(2*np.pi*6*f0*time_vector))\n",
    "\n",
    "# filter and normalize the excitation signal\n",
    "excitation = hf.butter_lowpass_filter(excitation, sampling_rate, cutoff=2000, order=3)\n",
    "excitation = hf.normalize(excitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a spectrogram of the excitation:\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.specgram(excitation/excitation.max(), NFFT=1024, Fs=44100, mode='magnitude', scale='dB', vmax=-20, vmin=-150)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.ylim(20,15000)\n",
    "plt.colorbar()\n",
    "# export the excitation signal as a wave file and play it back via IPython\n",
    "hf.write_wav(excitation, 'output/excitation.wav', 44100)\n",
    "Audio(\"output/excitation.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down your answer here:\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Cross-fading and evaluation\n",
    "---\n",
    "For a continuous auralization it is required to implement cross-fading between each block to avoid artifacts. Otherwise the discontinuities in the discretely sampled HRTF set will be clearly audible as jumps in the perceived position of the airplane.\n",
    "### Task 4.1: Dividing the signal into blocks\n",
    "The function `create_frames(input_sequence, hop, window_size)` in the cell below enables to split the `input_sequence` into frames of a given `window_size`. The function description contains a visual explanation of the process, which might be helpful for understanding the procedure.\n",
    "\n",
    "*Note: You are not supposed do do any implementation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frames(input_sequence, hop, window_size):\n",
    "    \"\"\"\n",
    "    Splits the input sequence in overlapping frames and\n",
    "    stores these frames into a matrix.\n",
    "\n",
    "    |------------------Input vector-----------------------|\n",
    "\n",
    "    |------1------|\n",
    "    |-hop-|------2------|\n",
    "                |------3------|\n",
    "                      |------4------|\n",
    "                            ...\n",
    "\n",
    "    Index            Frame\n",
    "      1         |------1------|\n",
    "      2         |------2------|\n",
    "      3         |------3------|\n",
    "      4         |------4------|\n",
    "     ...              ...\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_sequence : numpy.ndarray\n",
    "        Input sequence array.\n",
    "    hop : integer\n",
    "        Hopsize between consecutive frames.\n",
    "    window_size : integer\n",
    "        Size of each frame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    frames : numpy.ndarray\n",
    "        Output array containing the frames.\n",
    "    \"\"\"\n",
    "\n",
    "    # find the maximum number of slices that can be obtained\n",
    "    number_slices = int(np.floor((input_sequence.size-window_size)/hop))\n",
    "\n",
    "    # truncate if needed to get only an integer number of hop\n",
    "    input_sequence = input_sequence[0:int(number_slices*hop+window_size)];\n",
    "\n",
    "    # create a matrix with time slices\n",
    "    frames = np.zeros((int(np.floor(input_sequence.size/hop)), window_size))\n",
    "    \n",
    "    # fill the matrix\n",
    "    for idx in range (0,number_slices):\n",
    "        index_time_start = int(idx*hop)\n",
    "        index_time_end = int(idx*hop + window_size)\n",
    "        frames[idx,:] = input_sequence[index_time_start:index_time_end]\n",
    "        \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Selecting the HRIRs and implementing the cross-fading between the processed blocks\n",
    "---\n",
    "Implement the convolution with the HRIRs and the cross-fading between the convolved blocks.\n",
    "\n",
    "1. Use the function `create_frames` to create the frames and store them in the array `blocks`. The window size corresponds to the block size multiplied with the overlap factor, which is already defined in the cell below. \n",
    "\n",
    "2. Complete the main processing loop. Select the respective HRIR and apply the inverse distance law to the current frame.\n",
    "\n",
    "3. Apply a Hanning window to the frame (cross-fade) using the predefined array `hanning`.\n",
    "\n",
    "4. Convolve the frame with the respective HRIR. For this use the function `signal.oaconvolve(...)` ([Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.oaconvolve.html)) and store the results in two arrays. *Hint: Use 'same' as mode and specify the correct axis.* In advance, you need to stack the monaural input to a \"double-mono\" array using `np.vstack(...)` ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html)). You may copy your code from part A of the lab to do this.\n",
    "\n",
    "5. Add the result to the output array using an overlap-add principle. Pay attention to the correct indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "# initialize progress bar:\n",
    "progress_bar = IntProgress(min=0, max=int(n_blocks)-1, description='Processing:')\n",
    "display(progress_bar)\n",
    "\n",
    "# define overlap factor for windowing:\n",
    "overlap_factor = 4\n",
    "\n",
    "# pre-define hanning window:\n",
    "hanning = np.hanning(overlap_factor * blocksize)\n",
    "\n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "# create the overlapping blocks:\n",
    "# blocks = ...\n",
    "\n",
    "###### ! Solution ends here ! ######\n",
    "\n",
    "\n",
    "# initialize the output matrix to store the output blocks in\n",
    "out = np.zeros([2, sampling_rate*(duration+1)])\n",
    "\n",
    "# main block processing loop\n",
    "for idx in range(0, int(n_blocks)-1):\n",
    "    \n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "    # select the respective HRIRs:    \n",
    "    # HRIR = ...\n",
    "    \n",
    "    # select the start index of the current block:       \n",
    "    start = int((idx)*blocksize)\n",
    "    \n",
    "    # select the current input frames and apply the hanning window and also the 1/r law:           \n",
    "    \n",
    "    # ...\n",
    "    # input_frame_windowed = ...\n",
    "    \n",
    "    # convolve the input frames with the respective HRIR    \n",
    "    # direct = ...\n",
    "\n",
    "    # apply the overlay add operation\n",
    "    # out[:,start:start+direct.shape[-1]] = ...\n",
    "    \n",
    "###### ! Solution ends here ! ######\n",
    "\n",
    "    progress_bar.value += 1\n",
    "\n",
    "#normalize the output matrix\n",
    "auralization_without_reflection = hf.normalize(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.3: Playback and evaluation of audiblity\n",
    "---\n",
    "Evaluate the plausibility of the auralization and write down your observations. Name possibilities to improve the result. Which acoustical phenomena are not considered yet?\n",
    "\n",
    "Try to find an explanation for the steplike artifacts that can be obtained from the spectrogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.specgram(auralization_without_reflection[0], NFFT=1024,Fs=44100, mode='magnitude', scale='dB', vmax=-20, vmin=-180);\n",
    "plt.colorbar()\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.ylim(20,15000)\n",
    "hf.write_wav(auralization_without_reflection,'output/auralization_without_reflection.wav', 44100)\n",
    "Audio(\"output/auralization_without_reflection.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down your answer here:\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Floor-Reflection and the Doppler Shift\n",
    "---\n",
    "In this part, a floor reflection is considered by adding a source mirrored at the xy-plane. This method is commonly known as the image source  or mirror source algorithm. Later, a simplified auralization of the Doppler shift is given and evaluated.\n",
    "\n",
    "## Task 1: Calculateing the positions, distances, azimuth and elevation angles of the mirrored source\n",
    "---\n",
    "1. Calculate the posisitons of the image source by mirroring it at the xy-plane and store them in the array `mirror_src_pos`. The image source representing the reflection. The position of the image source is calculated using a transformation vector and performed as an elementwise multiplication with `src_pos`. See code below.\n",
    "\n",
    "2. Calculate the distances of the receiver and the positions of the mirror source using the euclidean norm, provided by `np.linalg.norm()` ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)) and store them in the array `mirror_distances`.\n",
    "\n",
    "3. Use the previously implemented fuction `angle_between_vectors(v1, v2)` to calculate the azimuth and elevation angles beween the up/view-vector and the source position vectors. A simple way to do this is to project the vectors to the xy- and xz-plane respectively and use the resulting two-dimensional vectors to calculate the angle. Store the results in the arrays `mirror_azimuth` and `mirror_elevation`.\n",
    "\n",
    "4. Calculate the time difference by subtracting the two distance vectors and applying the speed of sound. Store the absolute value of the result in the array `time_difference`.\n",
    "\n",
    "*Note: The resulting angles are only valid for the static receiver position. Due to the scope of this lab, a valid implementation of the rotation of the listener is omitted.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ! Solution begins here ! ######\n",
    "\n",
    "# 1. calculate the positions of the source, mirrored at the floor (xy-plane):\n",
    "transformation_vector = np.array((1,1,-1))\n",
    "mirror_src_pos = src_pos * transformation_vector\n",
    "\n",
    "# 2. calculate the distances of the mirror source to the receiver\n",
    "# mirror_distances = ...\n",
    "\n",
    "# projection of vectors to the xy and yz plane\n",
    "# ...\n",
    "\n",
    "# 3. calculate the azimuth and elevation angles \n",
    "# ...\n",
    "\n",
    "# 4. calculate the delay beween direct and reflected sound wave (t=s/v)\n",
    "# ...\n",
    "\n",
    "###### ! Solution ends here ! ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Cross-fading and evaluation\n",
    "---\n",
    "Extend the cross-fading loop and consinder the reflection.\n",
    "\n",
    "### Task 2.1: Selecting the HRIRs and implementing the cross-fading between the processed blocks\n",
    "---\n",
    "Implement the convolution with the HRIRs and the cross-fading between the convolved blocks.\n",
    "\n",
    "1. Use the implemented code from task 4.2 of the previous part and extend it in order to process the floor reflection.\n",
    "\n",
    "2. The floor reflection arrives later at the receiver than the direct sound. Thus, a delay is neccessary. Use the vector `time_difference` to calculate the number of samples for the delay. You are not expected to do an interpolation between the samples. For this task it is sufficient to floor the delay to integers.\n",
    "\n",
    "*Hint: Use the function np.pad(...) ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.pad.html)) to implement the delay. Also, the axes of the resulting arrays need to be aligned in order to perform elementswise calculations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize progress bar:\n",
    "progress_bar = IntProgress(min=0, max=int(n_blocks)-1, description='Processing:')\n",
    "display(progress_bar)\n",
    "\n",
    "# define overlap factor for windowing:\n",
    "overlap_factor = 4\n",
    "\n",
    "# pre-define hanning window:\n",
    "hanning = np.hanning(overlap_factor * blocksize)\n",
    "\n",
    "# create the overlapping blocks:\n",
    "blocks = create_frames(excitation, blocksize, blocksize*overlap_factor)\n",
    "\n",
    "# initialize the output matrix to store the output blocks in\n",
    "out = np.zeros([2, sampling_rate*(duration+1)])\n",
    "\n",
    "# main block processing loop\n",
    "for idx in range(0, int(n_blocks)-1):  \n",
    "    \n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "    # select the respective HRIRs:    \n",
    "    HRIR = hf.get_HRIR_at_direction(HRIR_dataset, azimuth[int(idx)], elevation[int(idx)])\n",
    "    # mirror_HRIR = ...\n",
    "    \n",
    "    # select the start index of the current block:       \n",
    "    start = int((idx)*blocksize)\n",
    "\n",
    "    # select the current input frame \n",
    "    # ...\n",
    "    \n",
    "    # apply the hanning window and at the same time the 1/r law:               \n",
    "    # input_frame_windowed = ...\n",
    "    # mirror_input_frame_windowed = ...\n",
    "    \n",
    "    # convolve the mirror input frames with the respective HRIR    \n",
    "    # ...\n",
    "        \n",
    "    # convolve the input frames with the respective HRIR    \n",
    "    # ...\n",
    "    \n",
    "    # calculate the delay of the reflection in samples:\n",
    "    shift = int(np.floor(time_difference[idx]*sampling_rate))\n",
    "\n",
    "    # apply the overlap add operation separately for each the direct sound and the reflection\n",
    "    # out[:,start:start+direct.shape[-1]] = ...\n",
    "    # out[:,start+shift:start+shift+reflection.shape[-1]] = ...\n",
    "\n",
    "###### ! Solution ends here ! ######\n",
    "\n",
    "    progress_bar.value += 1\n",
    "\n",
    "#normalize the output matrix\n",
    "auralization_with_reflection = hf.normalize(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Playback and evaluation of audiblity\n",
    "---\n",
    "Compare the resulting audio with the previous implementation and write down your observations. Is there a change in  perceived distance when adding reflections? Try to find an explanation for the audible effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.specgram(auralization_with_reflection[0], NFFT=1024,Fs=44100, scale='dB', vmin=-180)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.ylim(20,15000)\n",
    "hf.write_wav(auralization_with_reflection, 'output/auralization_with_reflection.wav', 44100)\n",
    "Audio(\"output/auralization_with_reflection.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down your answer here:\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Doppler shift and evaluation\n",
    "---\n",
    "This task gives a simple auralization of the Doppler shift.\n",
    "### Task 3.1: Playback and evaluation of audiblity\n",
    "---\n",
    "Compare the resulting audio with the previous implementation and write down your observations. Is the plausibility of the auralization enhanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the doppler effect by using linear interpolation and the delay\n",
    "src_pos_cont = start_point+np.outer(time_vector, velocity)\n",
    "distances_cont = np.linalg.norm(src_pos_cont - rec_pos, axis=-1)\n",
    "delay_cont = distances_cont / c\n",
    "auralization_with_doppler_shift = hf.linear_interpolation(\n",
    "    auralization_with_reflection[:,0:delay_cont.shape[0]], delay_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.specgram(auralization_with_doppler_shift[0], NFFT=1024,Fs=44100, scale='dB', vmin=-180)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.ylim(20,15000)\n",
    "hf.write_wav(auralization_with_doppler_shift, 'output/auralization_with_doppler_shift.wav', 44100)\n",
    "Audio(\"output/auralization_with_doppler_shift.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down your answer here:\n",
    "\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}